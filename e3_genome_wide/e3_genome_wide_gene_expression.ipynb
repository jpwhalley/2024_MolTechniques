{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGPS Molecular Techniques\n",
    "## Exercise 3: Working with genome-wide gene expression data\n",
    "### Comparing gene expression of naive, LPS and IFNy treated monocytes.\n",
    "+ Data from this paper:\n",
    "+ Fairfax, BP et al. Innate Immune Activity Conditions the Effect of Regulatory Variants upon Monocyte Gene Expression. Science 343,1246949(2014).\n",
    "#### First look: just one gene - TNF from the genome wide expression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in\n",
    "expr_all = pd.read_csv('monocyte_all_expression.csv', index_col=0)\n",
    "expr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract TNF data and the treatments\n",
    "expr_tnf = expr_all[['TNF']].copy()\n",
    "expr_tnf['Treatment'] = [idx.split('_')[-1] for idx in expr_tnf.index]\n",
    "\n",
    "# Group by treatment and calculate mean and standard error\n",
    "tnf_grouped = expr_tnf.groupby('Treatment').agg(['mean', 'std'])\n",
    "tnf_grouped.columns = ['Mean', 'StDev']\n",
    "\n",
    "# Number of replicates (assuming it's 3 for this example)\n",
    "n_replicates = 3\n",
    "tnf_grouped['StErr'] = tnf_grouped['StDev'] / np.sqrt(n_replicates)\n",
    "\n",
    "# Define the correct order and colors\n",
    "labels = ['Untreated', 'IFN', 'LPS2h', 'LPS24h']\n",
    "colours = ['red', 'green', 'blue', 'gold']\n",
    "\n",
    "# Reorder the dataframe according to the specified order\n",
    "tnf_grouped = tnf_grouped.reindex(labels)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(labels, \n",
    "       tnf_grouped['Mean'], \n",
    "       yerr=tnf_grouped['StErr'], \n",
    "       color=colours, \n",
    "       capsize=5)\n",
    "\n",
    "ax.set_xlabel('Treatment')\n",
    "ax.set_ylabel('TNF Expression')\n",
    "ax.set_title('TNF Gene Expression Across Treatments with StErr')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with all the gene expression data\n",
    "## Looking at dimension reduction, using principal component analysis\n",
    "\n",
    "In this dataset we have:\n",
    "    - 228 donors, ~15000 gene expression values\n",
    "\n",
    "The gene expression is from the innate immune cells, monocytes under 4 conditions:\n",
    "    - Untreated, as a control.\n",
    "    - Treated with inteferon-gamma (IFN) for 24 hours â€“ a good model for viral infections.\n",
    "    - Treated with Lipopolysaccharide (LPS) for 2 hours - LPS is a major component of the outer wall of gram negative bacteria, which our body registers as a toxin and elicits a strong immune response.\n",
    "    - Treated with Lipopolysaccharide (LPS) for 24 hours.\n",
    "    \n",
    "So we have a dataset for 912 samples (from 228 donors for 4 conditions each), gene expression data for ~15,000 genes. \n",
    "\n",
    "How to understand this dataset?\n",
    "\n",
    "No doubt there is high redundancy amongst the samples, so reducing them from ~15000 to a smaller number could be really helpful into interpreting the dataset (in this case for projecting the gene expression and genes into one value for each sample for each principal component).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data in\n",
    "expr_all = pd.read_csv('monocyte_all_expression.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what it looks like\n",
    "expr_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the treatment for each sample\n",
    "y = []\n",
    "for item in expr_all.index.values:\n",
    "    if 'Untreated' in item:\n",
    "        y.append(0)\n",
    "    elif 'IFN' in item:\n",
    "        y.append(1)\n",
    "    elif 'LPS2h' in item:\n",
    "        y.append(2)\n",
    "    elif 'LPS24h' in item:\n",
    "        y.append(3)\n",
    "    else:\n",
    "        print('Error: Data from non-recognisable experiment')\n",
    "y = np.array(y)\n",
    "\n",
    "labels = ['Untreated', 'IFN', 'LPS 2h', 'LPS 24h']\n",
    "colours = ['red', 'green', 'blue', 'gold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 1 and 2\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(expr_all)\n",
    "var_expl = pca.explained_variance_ratio_\n",
    "r_col = []\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('1st Comp: ' + str(round(var_expl[0]*100,1)) + '% variance explained')\n",
    "plt.ylabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp1_comp2.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 2 and 3\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 1], X_pca[y == i, 2], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "plt.ylabel('3rd Comp: ' + str(round(var_expl[2]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp2_comp3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of component 3 and 4\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(colours, range(len(labels)), labels):\n",
    "    plt.scatter(X_pca[y == i, 2], X_pca[y == i, 4], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('3rd Comp: ' + str(round(var_expl[2]*100,1)) + '% variance explained')\n",
    "plt.ylabel('4th Comp: ' + str(round(var_expl[3]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp2_comp3.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loadings for the first two components\n",
    "loadings = pca.components_.T\n",
    "\n",
    "# Creating the correct dataframe for loadings\n",
    "loadings_df = pd.DataFrame(loadings, index=expr_all.columns, columns=[f'PC{i+1}' for i in range(loadings.shape[1])])\n",
    "\n",
    "# Get the top and bottom 100 genes for PC1 and PC2\n",
    "top_100_pc1 = loadings_df['PC1'].nlargest(100)\n",
    "bottom_100_pc1 = loadings_df['PC1'].nsmallest(100)\n",
    "\n",
    "for gene in bottom_100_pc1.index.values:\n",
    "    print(gene)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can we see what gene pathways this list is enriched in?\n",
    "\n",
    "Try pasting these genes into Enrichr - https://maayanlab.cloud/Enrichr/\n",
    "and look at:\n",
    "\n",
    "+ Ontologies -> GO Biological Process\n",
    "+ Pathways -> KEGG\n",
    "\n",
    "If a particaular KEGG pathway is of interest, can also go further and look at it in KEGG: https://www.genome.jp/kegg/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the data\n",
    "\n",
    "From the PCA (well for the first-  three components), we can see a good separation between the four different treatments of the cells. Can we use clustering methods to fully classify them?\n",
    "\n",
    "Read through this tutorial - https://realpython.com/k-means-clustering-python/ and have a go at clustering the PCA data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set K-means for 4 clusters\n",
    "kmeans = KMeans(\n",
    "    init=\"random\",\n",
    "    n_clusters=4,\n",
    "    n_init=10,\n",
    "    max_iter=300,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_pca[:,:3])\n",
    "kmeans.inertia_\n",
    "kmeans.cluster_centers_\n",
    "# Counter(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the clusters on our plot\n",
    "new_colours = ['aqua', 'coral', 'yellow', 'seagreen']\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(new_colours, range(4), range(4)):\n",
    "    plt.scatter(X_pca[kmeans.labels_ == i, 0], X_pca[kmeans.labels_ == i, 1], s= 8, color=colour, lw=2, label=target_name)\n",
    "    plt.scatter(kmeans.cluster_centers_[i][0], kmeans.cluster_centers_[i][1],s=58, color='black', marker='x')\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('1st Comp: ' + str(round(var_expl[0]*100,1)) + '% variance explained')\n",
    "plt.ylabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp1_comp2_with_clusters.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of relying on th PCA, can we use clustering methods on all the data to fully classify them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(expr_all)\n",
    "Counter(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how the clusters on our plot\n",
    "new_colours = ['teal', 'orangered', 'goldenrod', 'chartreuse']\n",
    "fig = plt.figure(figsize=(18,12))\n",
    "for colour, i, target_name in zip(new_colours, range(4), range(4)):\n",
    "    plt.scatter(X_pca[kmeans.labels_ == i, 0], X_pca[kmeans.labels_ == i, 1], s= 8, color=colour, lw=2, label=target_name)\n",
    "\n",
    "plt.title(\"PCA of monocytes\")\n",
    "plt.legend(loc=1, shadow=False)\n",
    "# plt.axis([-4, 4, -1.5, 1.5])\n",
    "plt.xlabel('1st Comp: ' + str(round(var_expl[0]*100,1)) + '% variance explained')\n",
    "plt.ylabel('2nd Comp: ' + str(round(var_expl[1]*100,1)) + '% variance explained')\n",
    "fig.savefig('PCA_Extreme_Data_comp1_comp2_with_clusters.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
